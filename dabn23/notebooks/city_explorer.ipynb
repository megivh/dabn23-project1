{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d2d81ad",
   "metadata": {},
   "source": [
    "# City Explorer: Multi-Source Attraction & Activity Discovery with Routing\n",
    "\n",
    "A DABN 23 project by Alessandro Hoefer and Samuel Goldbuch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4013b173",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "When resorting to scraping techniques, the challenge is rarely the absence of data. Data online is abundant and easily accessible. However, the main difficulty lies in navigating such vast amounts of information and organizing it effectively.\n",
    "\n",
    "Tourism has grown steadily over recent years, with record numbers of people traveling abroad to explore new cities and countries. Yet for both groups and individuals, deciding what to do during a visit can be frustrating. Data on attractions, activities, optimal visiting times, and local tips overflow the internet, but putting order to this chaos remains challenging. These considerations motivated our project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7367a2",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "\n",
    "The goal of our script is to create a structured dataset, dynamically generated on demand, containing for any requested city: 10 must-see attractions, 10 potential activities, and real-time busyness data for those attractions.\n",
    "\n",
    "This could serve as the foundation for a consumer product that dynamically provides users with suggestions on what to do in a city. It offers a structured method to review available attractions and, if users are already at their destination, allows them to explore in real time which places to visit or avoid based on current crowding. Conceptually, the project divides into two complementary sections:\n",
    "- **Permanent storage of data**: Data is stored for long-term use through API access (see sections on TripAdvisor and Google Maps APIs). This primarily refers to the SQL databases that we create.\n",
    "- **Live data retrieval**: Data is accessed in real time for immediate use by potential users. This primarily refers to Selenium-based scraping for live crowdedness information.\n",
    "\n",
    "The project itself is divided into three main components that together achieve the overall vision:\n",
    "\n",
    "- *Google Maps APIs* for retrieval of static data on attractions in a given city\n",
    "- *TripAdvisor APIs* for retrieval of static data on activities in a given city\n",
    "- *Selenium library* for retrieval of real-time crowdedness data for each attraction in our database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3435a9",
   "metadata": {},
   "source": [
    "## Google Maps API Implementation\n",
    "In the project, cities are dynamically input into the Python code using an interactive UI. An input field is displayed through the IPython library using the display function, and an interactive button triggers the search. These two elements work in conjunction to pass city names to the integrated Google Maps API.\n",
    "\n",
    "The Google Maps API fetches data on the city's attractions, including:\n",
    "- Name\n",
    "- Address\n",
    "- Rating\n",
    "- Review count\n",
    "- Attraction category\n",
    "- Website\n",
    "- Phone\n",
    "\n",
    "This data is then stored in two ways:\n",
    "1.\tData is first stored in an SQL database for long-term persistence\n",
    "2.\tData is cached in the running Python instance for short-term usage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2ab4bc",
   "metadata": {},
   "source": [
    "## TripAdvisor API Implementation\n",
    "Similar to the Google Maps implementation, we use an interactive approach to input a city, which is then passed to create a query for the TripAdvisor API.\n",
    "\n",
    "The TripAdvisor API fetches data on the city's potential activities (things to do), including:\n",
    "- Name\n",
    "- Address\n",
    "- Rating\n",
    "- Review count\n",
    "- Activity category\n",
    "- Website\n",
    "\n",
    "Mirroring the Google Maps implementation, data is stored in dual fashion: in a separate SQL database for long-term use and cached in the Python instance for short-term access.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b383bc6",
   "metadata": {},
   "source": [
    "## Run through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9831d8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required packages are installed.\n"
     ]
    }
   ],
   "source": [
    "# 0) Dependency check (optional)\n",
    "# This notebook does NOT auto-install by default (cleaner + more reproducible).\n",
    "AUTO_INSTALL = False\n",
    "\n",
    "required = [\n",
    "    (\"requests\", \"requests\"),\n",
    "    (\"pandas\", \"pandas\"),\n",
    "    (\"ipywidgets\", \"ipywidgets\"),\n",
    "]\n",
    "\n",
    "missing = []\n",
    "for import_name, pip_name in required:\n",
    "    try:\n",
    "        __import__(import_name)\n",
    "    except ImportError:\n",
    "        missing.append(pip_name)\n",
    "\n",
    "if missing:\n",
    "    print(\"Missing packages:\", \", \".join(missing))\n",
    "    print(\"Install command:\")\n",
    "    print(\"  pip install \" + \" \".join(missing))\n",
    "    if AUTO_INSTALL:\n",
    "        import sys, subprocess\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *missing])\n",
    "        print(\"Installed. Re-run this cell if needed.\")\n",
    "else:\n",
    "    print(\"All required packages are installed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8dc7d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\Users\\Samuel\\Desktop\\Git Repo\\dabn23-project1\\dabn23\n"
     ]
    }
   ],
   "source": [
    "# 1) Make sure we can import from /src (works when running from notebooks/ folder)\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(\"Project root:\", PROJECT_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db4021eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google API key loaded (length): 39\n",
      "TripAdvisor API key loaded (length): 32\n",
      "DB_PATH: G:\\My Drive\\dabn23_SharedDatabase\\dabn23_places_cache.sqlite\n"
     ]
    }
   ],
   "source": [
    "# 2) Load configuration (API keys + DB path)\n",
    "# config.py fails fast with a helpful error message if something is missing.\n",
    "\n",
    "from src.config import GOOGLE_API_KEY, TA_API_KEY, DB_PATH\n",
    "\n",
    "print(\"Google API key loaded (length):\", len(GOOGLE_API_KEY))\n",
    "print(\"TripAdvisor API key loaded (length):\", len(TA_API_KEY))\n",
    "print(\"DB_PATH:\", DB_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56900629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DB ready. Tables: ['city_top10', 'item_summary']\n"
     ]
    }
   ],
   "source": [
    "# 3) Initialize the shared SQLite database (creates the file if it doesn't exist)\n",
    "\n",
    "from pathlib import Path\n",
    "from src.db import connect, migrate_if_needed, create_tables\n",
    "\n",
    "# Ensure parent folder exists (SQLite can create the file, but not the folder)\n",
    "Path(DB_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "conn = connect(DB_PATH)\n",
    "migrate_if_needed(conn)   # handles legacy schemas (e.g., place_ids_json -> item_ids_json)\n",
    "create_tables(conn)\n",
    "\n",
    "tables = conn.execute(\"SELECT name FROM sqlite_master WHERE type='table';\").fetchall()\n",
    "print(\"✅ DB ready. Tables:\", [t[0] for t in tables])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e21d9d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 definitions with imports\n",
    "\n",
    "from src.pipelines import top10_city\n",
    "\n",
    "ALLOW = [\"Tours\", \"Food & Drink\", \"Outdoor Activities\", \"Boat Tours & Water Sports\", \"Nightlife\", \"Shopping\"]\n",
    "DENY  = [\"Sights & Landmarks\", \"Museums\"]\n",
    "\n",
    "def city_search(city: str):\n",
    "    return top10_city(conn, city, allow_groups=ALLOW, deny_groups=DENY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce17d1e",
   "metadata": {},
   "source": [
    "## 5) Interactive search UI (ipywidgets)\n",
    "\n",
    "Use the controls to choose:\n",
    "- city\n",
    "- data source (Google or TripAdvisor)\n",
    "- type (attraction/activity)\n",
    "\n",
    "Then click **Search Top 10**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e10d46d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e5cd85a140a43a58ee8f592330355a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Text(value='Paris', description='City:', layout=Layout(width='420px'), placehold…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'last_city': None, 'last_results': None}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.ui import build_city_widget\n",
    "\n",
    "LAST_SEARCHED_CITY = None\n",
    "LAST_SEARCH_RESULTS = None\n",
    "\n",
    "build_city_widget(city_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59ab13c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(LAST_SEARCHED_CITY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3bdfd4",
   "metadata": {},
   "source": [
    "# Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b7ec1b",
   "metadata": {},
   "source": [
    "## Selenium for Real-Time Crowdedness Data\n",
    "To complement our permanent data collection through APIs, we use the Selenium package to add a real-time dimension to the data retrieval process.\n",
    "\n",
    "Specifically, we query the SQL database to retrieve all available attractions for a given city. Then, we use that information to sequentially navigate Google Maps and fetch real-time busyness data for each attraction. The data is returned in a tabular format for easy interpretation.\n",
    "\n",
    "From a product perspective, this is one of the most compelling features: potential users can check in real time which places to visit and which to avoid based on current crowding levels, helping them find the best spot at any given moment. However, this functionality depends entirely on the previously created SQL database containing all attraction information for a given city. There is a symbiotic relationship here, where long-term data storage enables live data retrieval and short-term usage of the product.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7aa713",
   "metadata": {},
   "source": [
    "## Selenium implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ef446ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sqlite3\n",
    "import datetime\n",
    "\n",
    "def get_attraction_names(city: str, conn: sqlite3.Connection):\n",
    "    \"\"\"\n",
    "    Look up stored attraction names for a city from city_top10 / ta_place_summary.\n",
    "    Returns a list of name strings, or None if city not found.\n",
    "    \"\"\"\n",
    "    citykey = city.strip().lower()\n",
    "\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Read the stored place_ids_json for this city\n",
    "    row = cur.execute(\n",
    "        \"SELECT item_ids_json FROM city_top10 \"\n",
    "        \"WHERE city_key = ? AND source = ? AND item_type = ?\",\n",
    "        (citykey, \"google\", \"attraction\")\n",
    "    ).fetchone()\n",
    "\n",
    "    if not row:\n",
    "        # City not in DB\n",
    "        return None\n",
    "\n",
    "    place_ids = json.loads(row[0])\n",
    "    if not place_ids:\n",
    "        return []\n",
    "\n",
    "    # Fetch names in the same ranked order\n",
    "    placeholders = \",\".join(\"?\" * len(place_ids))\n",
    "    name_rows = cur.execute(\n",
    "        f\"SELECT item_id, name FROM item_summary \"\n",
    "        f\"WHERE source = ? AND item_id IN ({placeholders})\",\n",
    "        [\"google\", *place_ids]\n",
    "    ).fetchall()\n",
    "\n",
    "    name_map = {pid: name for pid, name in name_rows}\n",
    "\n",
    "    # Preserve original ranking order\n",
    "    return [name_map[pid] for pid in place_ids if pid in name_map]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f1b1cfeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TripAdvisor DB path: c:\\Users\\Samuel\\Desktop\\Git Repo\\dabn23-project1\\dabn23\\notebooks\\dabn23_tripadvisor_cache.sqlite\n",
      "Exists? True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import pathlib\n",
    "\n",
    "# Path to TripAdvisor cache in the same folder as this notebook's working dir\n",
    "TA_DB_PATH = str(pathlib.Path().cwd() / \"dabn23_tripadvisor_cache.sqlite\")\n",
    "\n",
    "print(\"TripAdvisor DB path:\", TA_DB_PATH)\n",
    "print(\"Exists?\", os.path.exists(TA_DB_PATH))\n",
    "\n",
    "taconn = sqlite3.connect(TA_DB_PATH)\n",
    "#taconn.execute(\"PRAGMA journal_mode=WAL;\")  # safe even if wal/shm files present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "087a7fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_busy_bar(aria: str):\n",
    "    \"\"\"\n",
    "    Parses one peak-hours bar aria-label.\n",
    "    Handles English (\"77% busy at 2 pm\") and Swedish/Nordic (\"77 aktivitet kl. 1400.\").\n",
    "    Returns (hour_24, pct) or None.\n",
    "    \"\"\"\n",
    "    # Swedish/Nordic: \"37 aktivitet kl. 1300.\"\n",
    "    m = re.search(r\"^(\\d+)\\D+?kl\\.\\s*(\\d{2})\\d{2}\", aria.strip())\n",
    "    if m:\n",
    "        return int(m.group(2)), int(m.group(1))\n",
    "\n",
    "    # English: \"77% busy at 2 pm\"\n",
    "    m = re.search(r\"(\\d+)%.*?(\\d{1,2})\\s*(am|pm)\", aria, re.IGNORECASE)\n",
    "    if m:\n",
    "        pct, h, mer = int(m.group(1)), int(m.group(2)), m.group(3).lower()\n",
    "        hour_24 = (h % 12) + (12 if mer == \"pm\" else 0)\n",
    "        return hour_24, pct\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_current_busyness(driver, attraction_name: str):\n",
    "    \"\"\"\n",
    "    Searches Google Maps for attraction_name and scrapes the full-day busyness.\n",
    "    Returns list[int|None] with 24 entries (index = hour 0–23),\n",
    "    or None if the place has no peak-hours section at all.\n",
    "    \"\"\"\n",
    "    print(f\"\\n  Searching: {attraction_name}\")\n",
    "\n",
    "    # 1. Navigate and type in search bar\n",
    "    driver.get(\"https://www.google.com/maps\")\n",
    "    time.sleep(2)\n",
    "    dismiss_google_consent(driver)\n",
    "\n",
    "    search_bar = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.NAME, \"q\"))\n",
    "    )\n",
    "    search_bar.clear()\n",
    "    search_bar.send_keys(attraction_name)\n",
    "    driver.find_element(By.CSS_SELECTOR, \"button.mL3xi\").click()\n",
    "    time.sleep(3)\n",
    "\n",
    "    # 2. Disambiguation list → click first result if present\n",
    "    try:\n",
    "        WebDriverWait(driver, 5).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, \"a.hfpxzc\"))\n",
    "        ).click()\n",
    "        time.sleep(3)\n",
    "        print(\"    Clicked top result from list.\")\n",
    "    except TimeoutException:\n",
    "        print(\"    Landed directly on place page.\")\n",
    "\n",
    "    # 3. Find peak-hours section\n",
    "    try:\n",
    "        peak_section = WebDriverWait(driver, 6).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"div.UmE4Qe\"))\n",
    "        )\n",
    "    except TimeoutException:\n",
    "        print(\"    No peak hours data available.\")\n",
    "        return None\n",
    "\n",
    "    # 4. Parse all hourly bars into a 24-slot list\n",
    "    hourly_data = [None] * 24\n",
    "    bars = peak_section.find_elements(By.CSS_SELECTOR, \"div.dpoVLd\")\n",
    "    print(f\"    Found {len(bars)} hourly bars.\")\n",
    "\n",
    "    for bar in bars:\n",
    "        aria = bar.get_attribute(\"aria-label\") or \"\"\n",
    "\n",
    "        # Live \"Currently X% busy\" → slot into current hour\n",
    "        live = re.search(r\"(?:Currently|Nuvarande).*?(\\d+)%\", aria, re.IGNORECASE)\n",
    "        if live:\n",
    "            hourly_data[datetime.datetime.now().hour] = int(live.group(1))\n",
    "            print(f\"    ✓ Live now: {int(live.group(1))}%\")\n",
    "            continue\n",
    "\n",
    "        parsed = _parse_busy_bar(aria)\n",
    "        if parsed:\n",
    "            hour_24, pct = parsed\n",
    "            if 0 <= hour_24 <= 23:\n",
    "                hourly_data[hour_24] = pct\n",
    "\n",
    "    filled = sum(1 for x in hourly_data if x is not None)\n",
    "    print(f\"    ✓ Stored {filled}/24 hours.\")\n",
    "    return hourly_data\n",
    "\n",
    "\n",
    "def dismiss_google_consent(driver):\n",
    "    \"\"\"Dismiss the GDPR consent banner on Google Maps (EU only).\"\"\"\n",
    "    try:\n",
    "        accept_btn = WebDriverWait(driver, 8).until(\n",
    "            EC.element_to_be_clickable((\n",
    "                By.XPATH,\n",
    "                '//button[.//span[contains(text(),\"Accept all\") '\n",
    "                'or contains(text(),\"Reject all\")]]'\n",
    "            ))\n",
    "        )\n",
    "        accept_btn.click()\n",
    "        time.sleep(1)\n",
    "        print(\"  Google consent dismissed.\")\n",
    "    except:\n",
    "        print(\"  No consent popup found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a9472e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def scrape_peak_hours(city: str, conn: sqlite3.Connection):\n",
    "    \"\"\"\n",
    "    Scrapes Google Maps peak hours for all TA top-10 attractions of a city.\n",
    "    Saves results into the global `busyness_data` dict.\n",
    "    Supports multiple cities — each call adds/updates one city entry.\n",
    "    Prints the current-hour snapshot when done.\n",
    "    \"\"\"\n",
    "    scraped_at = datetime.datetime.now().strftime(\"%H:%M\")\n",
    "    city_key   = city.strip()\n",
    "\n",
    "    print(f\"Looking up '{city_key}' in  DB...\")\n",
    "    names = get_attraction_names(city_key, conn)\n",
    "\n",
    "    if names is None:\n",
    "        print(f\"  ✗ '{city_key}' not found in DB. Run the scraper first.\")\n",
    "        return\n",
    "    if not names:\n",
    "        print(f\"  ✗ No attractions stored for '{city_key}'.\")\n",
    "        return\n",
    "\n",
    "    print(f\"  Found {len(names)} attractions: {', '.join(names[:3])}...\")\n",
    "\n",
    "    attractions = {}\n",
    "    try:\n",
    "        for name in names:\n",
    "            hourly = get_current_busyness(driver, name)\n",
    "            attractions[name] = hourly   # list[int|None] or None\n",
    "            time.sleep(2)\n",
    "    finally:\n",
    "        print(\"\\nDriver closed.\")\n",
    "\n",
    "    # Save into global dict (safe to call again for a different city)\n",
    "    busyness_data[city_key] = {\n",
    "        \"scraped_at\":  scraped_at,\n",
    "        \"attractions\": attractions,\n",
    "    }\n",
    "\n",
    "    # Print current-hour snapshot\n",
    "    now_hour = datetime.datetime.now().hour\n",
    "    print(f\"\\n{'='*54}\")\n",
    "    print(f\"  CURRENT BUSYNESS — {city_key}  (scraped at {scraped_at})\")\n",
    "    print(f\"{'='*54}\")\n",
    "    for name, hourly in attractions.items():\n",
    "        if hourly is None:\n",
    "            val = \"N/A (no GM data)\"\n",
    "        elif hourly[now_hour] is None:\n",
    "            val = \"N/A (no data this hour)\"\n",
    "        else:\n",
    "            val = f\"{hourly[now_hour]}%\"\n",
    "        print(f\"  {name:<44} {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9225ee6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver started.\n",
      "Looking up 'Stockholm' in  DB...\n",
      "  Found 10 attractions: Vasa Museum, The Royal Palace, Skansen...\n",
      "\n",
      "  Searching: Vasa Museum\n",
      "  Google consent dismissed.\n",
      "    Landed directly on place page.\n",
      "    Found 126 hourly bars.\n",
      "    ✓ Stored 18/24 hours.\n",
      "\n",
      "  Searching: The Royal Palace\n",
      "  No consent popup found.\n",
      "    Clicked top result from list.\n",
      "    No peak hours data available.\n",
      "\n",
      "  Searching: Skansen\n",
      "  No consent popup found.\n",
      "    Landed directly on place page.\n",
      "    Found 126 hourly bars.\n",
      "    ✓ Stored 18/24 hours.\n",
      "\n",
      "  Searching: King's Garden\n",
      "  No consent popup found.\n",
      "    Clicked top result from list.\n",
      "    Found 168 hourly bars.\n",
      "    ✓ Stored 24/24 hours.\n",
      "\n",
      "  Searching: ABBA The Museum\n",
      "  No consent popup found.\n",
      "    Landed directly on place page.\n",
      "    Found 126 hourly bars.\n",
      "    ✓ Stored 18/24 hours.\n",
      "\n",
      "  Searching: Fotografiska Museum Stockholm\n",
      "  No consent popup found.\n",
      "    Landed directly on place page.\n",
      "    Found 126 hourly bars.\n",
      "    ✓ Live now: 100%\n",
      "    ✓ Stored 18/24 hours.\n",
      "\n",
      "  Searching: Swedish History Museum\n",
      "  No consent popup found.\n",
      "    Landed directly on place page.\n",
      "    Found 108 hourly bars.\n",
      "    ✓ Stored 18/24 hours.\n",
      "\n",
      "  Searching: Stockholm City Hall\n",
      "  No consent popup found.\n",
      "    Landed directly on place page.\n",
      "    Found 126 hourly bars.\n",
      "    ✓ Stored 18/24 hours.\n",
      "\n",
      "  Searching: Skyview\n",
      "  No consent popup found.\n",
      "    Clicked top result from list.\n",
      "    No peak hours data available.\n",
      "\n",
      "  Searching: Storkyrkan\n",
      "  No consent popup found.\n",
      "    Landed directly on place page.\n",
      "    Found 126 hourly bars.\n",
      "    ✓ Stored 18/24 hours.\n",
      "\n",
      "Driver closed.\n",
      "\n",
      "======================================================\n",
      "  CURRENT BUSYNESS — Stockholm  (scraped at 20:36)\n",
      "======================================================\n",
      "  Vasa Museum                                  0%\n",
      "  The Royal Palace                             N/A (no GM data)\n",
      "  Skansen                                      0%\n",
      "  King's Garden                                60%\n",
      "  ABBA The Museum                              0%\n",
      "  Fotografiska Museum Stockholm                53%\n",
      "  Swedish History Museum                       0%\n",
      "  Stockholm City Hall                          0%\n",
      "  Skyview                                      N/A (no GM data)\n",
      "  Storkyrkan                                   0%\n",
      "Finished scraping.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import re\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--lang=en-US\")\n",
    "options.add_argument(\"--headless=new\")\n",
    "options.add_experimental_option(\"prefs\", {\n",
    "    \"intl.accept_languages\": \"en-US,en\",\n",
    "    \"profile.default_content_setting_values.geolocation\": 2,\n",
    "})\n",
    "options.add_argument(\n",
    "    \"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "    \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    ")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.execute_cdp_cmd(\"Emulation.setGeolocationOverride\", {\n",
    "    \"latitude\": 40.7128, \"longitude\": -74.0060, \"accuracy\": 100\n",
    "})\n",
    "print(\"Driver started.\")\n",
    "\n",
    "# Global storage for peak-hours data (multi-city)\n",
    "busyness_data = {}\n",
    "scrape_peak_hours(\"Stockholm\", conn)\n",
    "\n",
    "driver.close()\n",
    "print(\"Finished scraping.\")\n",
    "# To scrape additional cities, re-run this cell with a new city name.\n",
    "# busyness_data will accumulate entries for all cities scraped this session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "04b3becb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Stockholm  (scraped at 20:36) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attraction</th>\n",
       "      <th>Busy at 20:00</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vasa Museum</td>\n",
       "      <td>0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Royal Palace</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Skansen</td>\n",
       "      <td>0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>King's Garden</td>\n",
       "      <td>60%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA The Museum</td>\n",
       "      <td>0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fotografiska Museum Stockholm</td>\n",
       "      <td>53%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Swedish History Museum</td>\n",
       "      <td>0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Stockholm City Hall</td>\n",
       "      <td>0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Skyview</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Storkyrkan</td>\n",
       "      <td>0%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Attraction Busy at 20:00\n",
       "0                    Vasa Museum            0%\n",
       "1               The Royal Palace           N/A\n",
       "2                        Skansen            0%\n",
       "3                  King's Garden           60%\n",
       "4                ABBA The Museum            0%\n",
       "5  Fotografiska Museum Stockholm           53%\n",
       "6         Swedish History Museum            0%\n",
       "7            Stockholm City Hall            0%\n",
       "8                        Skyview           N/A\n",
       "9                     Storkyrkan            0%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def print_busyness_summary(city: str = None):\n",
    "    \"\"\"Print a DataFrame of current-hour busyness. Omit city= for all cities.\"\"\"\n",
    "    cities = [city] if city else list(busyness_data.keys())\n",
    "    now_hour = datetime.datetime.now().hour\n",
    "\n",
    "    if not cities:\n",
    "        print(\"busyness_data is empty — run scrape_peak_hours() first.\")\n",
    "        return\n",
    "\n",
    "    for c in cities:\n",
    "        if c not in busyness_data:\n",
    "            print(f\"No data for '{c}'.\")\n",
    "            continue\n",
    "        entry = busyness_data[c]\n",
    "        rows = []\n",
    "        for name, hourly in entry[\"attractions\"].items():\n",
    "            pct = None if (hourly is None) else hourly[now_hour]\n",
    "            rows.append({\n",
    "                \"Attraction\":           name,\n",
    "                f\"Busy at {now_hour:02d}:00\": f\"{pct}%\" if pct is not None else \"N/A\"\n",
    "            })\n",
    "        print(f\"\\n=== {c}  (scraped at {entry['scraped_at']}) ===\")\n",
    "        display(pd.DataFrame(rows))\n",
    "\n",
    "print_busyness_summary(\"Stockholm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5302b3fc",
   "metadata": {},
   "source": [
    "## Results and Limitations\n",
    "From an overall project results perspective, we are satisfied with the final outcomes.\n",
    "Following the implementation of the Google and TripAdvisor APIs, we successfully populated a solid SQL database. We felt that inserting a limited number of cities was sufficient for demonstration purposes, but the code is written in an interactive way such that, on demand, we can integrate our database with additional information for any desired city.\n",
    "We are also pleased with our implementation of Selenium to retrieve crowdedness information. Fetching data directly from the SQL database makes the process leaner and allows users to access this information at a later time, which is key to the system's functionality. Users can first explore attractions and activities, then decide later whether to run the Selenium script for real-time data. Fetching this data can be time-intensive, since we deliberately slowed the process to avoid potential scraping blocks, taking a conservative approach.\n",
    "\n",
    "This leads us to the project's current limitations.\n",
    "- First, rapid fetching of attraction crowdedness would be essential in a working product. Given the pedagogical nature of this project, we did not feel it was necessary to optimize speed or implement techniques to parallelize the process. However, we recognize this flaw, which would be critical to address for a deliverable product.\n",
    "- Second, crowdedness can only be checked for attractions in the current implementation. Since we use Google Maps to retrieve this information, we are limited to Google Maps attractions. Unfortunately, we cannot check activity crowdedness for TripAdvisor-sourced activities, as TripAdvisor does not supply users with such data.\n",
    "- Additionally, the current implementation requires using a VPN, connected to a country whose official language is english, to run the scraping script reliably. Google Maps interface language still changes based on geolocation, and simply adjusting the WebDriver headers has not been enough to override this behavior. In future iterations, more robust workarounds could be introduced, but given the course context and prototype nature of the project, we consider the present solution acceptable.\n",
    "- Finally, we must acknowledge the static nature of data inserted into the SQL database. For a more robust product, implementing a recurring update function would be fundamental. With the present structure, manually erasing all stored data and using the interactive UI to fetch it again would be necessary to 'update' the available information. This is costly, both computationally and time-wise. A better approach would be to implement regular checks to determine whether data has changed enough to warrant the expense of updating.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cded64",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "As a proof of concept for a product that could facilitate travel planning for tourists worldwide, we consider ourselves satisfied. The combined use of long-term data storage with short-term data retrieval through APIs and the Selenium package proved to be a powerful synergy, allowing flexibility, ease of access, and surprisingly decent timeliness. \n",
    "\n",
    "Of course, we acknowledge the aforementioned limitations. However, scaling this project with more computing power, parallelization of processes across multiple (virtual) machines, and strategic implementation of powerful libraries such as the Requests library could improve timeliness and make such a product a deliverable reality.\n",
    "\n",
    "Furthermore, we cannot ignore the pedagogical benefits of this project. Working with APIs and a robust package like Selenium on mainstream platforms such as Google Maps and TripAdvisor gave us valuable insights into data retrieval, web scraping, and the ideation of a working product. These are skills that are difficult to develop in an educational setting. We believe this experience will be highly beneficial for our professional growth.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
