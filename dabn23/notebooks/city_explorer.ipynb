{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d2d81ad",
   "metadata": {},
   "source": [
    "# City Explorer: Multi-Source Attraction & Activity Discovery with Routing\n",
    "\n",
    "A DABN 23 project by Alessandro Hoefer and Samuel Goldbuch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4013b173",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "When resorting to scraping techniques, the challenge is rarely the absence of data. Data online is abundant and easily accessible. However, the main difficulty lies in navigating such vast amounts of information and organizing it effectively.\n",
    "\n",
    "Tourism has grown steadily over recent years, with record numbers of people traveling abroad to explore new cities and countries. Yet for both groups and individuals, deciding what to do during a visit can be frustrating. Data on attractions, activities, optimal visiting times, and local tips overflow the internet, but putting order to this chaos remains challenging. These considerations motivated our project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7367a2",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "\n",
    "The goal of our script is to create a structured dataset, dynamically generated on demand, containing for any requested city: 10 must-see attractions, up to 10 potential activities, and real-time busyness data for those attractions.\n",
    "\n",
    "This could serve as the foundation for a consumer product that dynamically provides users with suggestions on what to do in a city. It offers a structured method to review available attractions and, if users are already at their destination, allows them to explore in real time which places to visit or avoid based on current crowding. Conceptually, the project divides into two complementary sections:\n",
    "- **Permanent storage of data**: Data is stored for long-term use through API access (see sections on TripAdvisor and Google Maps APIs). This primarily refers to the SQL databases that we create.\n",
    "- **Live data retrieval**: Data is accessed in real time for immediate use by potential users. This primarily refers to Selenium-based scraping for live crowdedness information.\n",
    "\n",
    "The project itself is divided into three main components that together achieve the overall vision:\n",
    "\n",
    "- *Google Maps APIs* for retrieval of static data on attractions in a given city\n",
    "- *TripAdvisor APIs* for retrieval of static data on activities in a given city\n",
    "- *Selenium library* for retrieval of real-time crowdedness data for each attraction in our database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3435a9",
   "metadata": {},
   "source": [
    "## Google Maps API Implementation\n",
    "In the project, cities are dynamically input into the Python code using an interactive UI. An input field is displayed through the IPython library using the display function, and an interactive button triggers the search. These two elements work in conjunction to pass city names to the integrated Google Maps API.\n",
    "\n",
    "The Google Maps API fetches data on the city's attractions, including:\n",
    "- Name\n",
    "- Address\n",
    "- Rating\n",
    "- Review count\n",
    "- Attraction category\n",
    "- Website\n",
    "- Phone\n",
    "\n",
    "This data is then stored in two ways:\n",
    "1.\tData is first stored in an SQL database for long-term persistence\n",
    "2.\tData is cached in the running Python instance for short-term usage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2ab4bc",
   "metadata": {},
   "source": [
    "## TripAdvisor API Implementation\n",
    "The input in the interactive UI is also forwarded to the TripAdvisor API, which is then passed to create a query for the fetched data.\n",
    "\n",
    "The TripAdvisor API fetches data on the city's potential activities (things to do), including:\n",
    "- Name\n",
    "- Address\n",
    "- Rating\n",
    "- Review count\n",
    "- Activity category\n",
    "- Website\n",
    "\n",
    "Mirroring the Google Maps implementation, data is stored in dual fashion: in a SQL database for long-term use and cached in the Python instance for short-term access.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b383bc6",
   "metadata": {},
   "source": [
    "## Notebook Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d5f85c",
   "metadata": {},
   "source": [
    "### 0) Dependency Check\n",
    "This code block verifies that required libraries (requests, pandas, ipywidgets) are installed in the notebook environment. Missing packages are flagged and accompanied by a pip install command, ensuring the system is properly configured before execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9831d8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required packages are installed.\n"
     ]
    }
   ],
   "source": [
    "AUTO_INSTALL = False\n",
    "\n",
    "required = [\n",
    "    (\"requests\", \"requests\"),\n",
    "    (\"pandas\", \"pandas\"),\n",
    "    (\"ipywidgets\", \"ipywidgets\"), \n",
    "    (\"selenium\", \"selenium\"),\n",
    "]\n",
    "\n",
    "missing = []\n",
    "for import_name, pip_name in required:\n",
    "    try:\n",
    "        __import__(import_name)\n",
    "    except ImportError:\n",
    "        missing.append(pip_name)\n",
    "\n",
    "if missing:\n",
    "    print(\"Missing packages:\", \", \".join(missing))\n",
    "    print(\"Install command:\")\n",
    "    print(\"  pip install \" + \" \".join(missing))\n",
    "    if AUTO_INSTALL:\n",
    "        import sys, subprocess\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *missing])\n",
    "        print(\"Installed. Re-run this cell if needed.\")\n",
    "else:\n",
    "    print(\"All required packages are installed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b749bf",
   "metadata": {},
   "source": [
    "### 1) Project Path Configuration\n",
    "\n",
    "This code block configures the notebook’s system path to ensure modules from the project’s `/src` directory can be imported correctly when executed from the `notebooks` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8dc7d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\Users\\megiv\\Desktop\\Git Clone\\dabn23-project1\\dabn23\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(\"Project root:\", PROJECT_ROOT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd19be3",
   "metadata": {},
   "source": [
    "### 2) Configuration Loading\n",
    "\n",
    "This code block imports API credentials and the database path from the centralized `config.py` file. Execution requires users to provide their own Google Maps and TripAdvisor API keys, as well as a valid connection to the SQL database stored on Google Drive, all configured as system variables. Successful loading confirms that external services and database access are properly configured before pipeline execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db4021eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google API key loaded (length): 39\n",
      "TripAdvisor API key loaded (length): 32\n",
      "DB_PATH: G:\\My Drive\\dabn23_SharedDatabase\\dabn23_cache.sqlite\n"
     ]
    }
   ],
   "source": [
    "# 2) Load configuration (API keys + DB path)\n",
    "# config.py fails fast with a helpful error message if something is missing.\n",
    "\n",
    "from src.config import GOOGLE_API_KEY, TA_API_KEY, DB_PATH\n",
    "\n",
    "print(\"Google API key loaded (length):\", len(GOOGLE_API_KEY))\n",
    "print(\"TripAdvisor API key loaded (length):\", len(TA_API_KEY))\n",
    "print(\"DB_PATH:\", DB_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37f3d0b",
   "metadata": {},
   "source": [
    "### 3) Database Initialization\n",
    "\n",
    "This code block initialises the shared SQLite database by establishing a connection, and creating all required tables. If the database file or its parent directory does not yet exist, they are created automatically. Successful execution confirms that the storage layer is fully configured and ready for data input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56900629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DB ready. Tables: ['city_top10', 'item_summary']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from src.db import connect, migrate_if_needed, create_tables\n",
    "\n",
    "# Ensure parent folder exists (SQLite can create the file, but not the folder)\n",
    "Path(DB_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "conn = connect(DB_PATH)\n",
    "migrate_if_needed(conn)   # handles legacy schemas (e.g., place_ids_json -> item_ids_json)\n",
    "create_tables(conn)\n",
    "\n",
    "tables = conn.execute(\"SELECT name FROM sqlite_master WHERE type='table';\").fetchall()\n",
    "print(\"✅ DB ready. Tables:\", [t[0] for t in tables])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7c149e",
   "metadata": {},
   "source": [
    "### 4) Pipeline Definition\n",
    "\n",
    "This code block imports the core city-level pipeline function and defines a filtered search wrapper for notebook execution. Activity categories are selectively included or excluded through predefined allow and deny lists. The resulting function enables streamlined retrieval of the project’s curated Top-10 outputs for any queried city. The goal of the allow and deny list was to use the TripAdvisor to gather addotional information regarding a cities activities in contrast to the attractions gathered from Google Maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e21d9d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 definitions with imports\n",
    "\n",
    "from src.pipelines import top10_city\n",
    "\n",
    "ALLOW = [\"Tours\", \"Food & Drink\", \"Outdoor Activities\", \"Boat Tours & Water Sports\", \"Nightlife\", \"Shopping\"]\n",
    "DENY  = [\"Sights & Landmarks\", \"Museums\"]\n",
    "\n",
    "def city_search(city: str):\n",
    "    return top10_city(conn, city, allow_groups=ALLOW, deny_groups=DENY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce17d1e",
   "metadata": {},
   "source": [
    "### 5) Interactive City Search UI\n",
    "\n",
    "This code block imports and initialises the interactive city search widget within the notebook environment. It connects the previously defined pipeline function to a user-facing input interface, enabling dynamic execution of the full data retrieval process for any entered city. Global variables store the most recent search and results for reuse data that is already stored in the database to save API tokens and improve performance. Less than 10 activities might appear for some cities because TripAdvisor's API limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e10d46d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b44e79552046a4ac1d9d3e89ecf584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Text(value='Paris', description='City:', layout=Layout(width='420px'), placehold…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from src.ui import build_city_widget\n",
    "\n",
    "LAST_SEARCHED_CITY = None\n",
    "LAST_SEARCH_RESULTS = None\n",
    "\n",
    "build_city_widget(city_search)\n",
    "print(LAST_SEARCHED_CITY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b7ec1b",
   "metadata": {},
   "source": [
    "## Selenium for Real-Time Crowdedness Data\n",
    "To complement our permanent data collection through APIs, we use the Selenium package to add a real-time dimension to the data retrieval process.\n",
    "\n",
    "Specifically, we query the SQL database to retrieve all available attractions for a given city. Then, we use that information to sequentially navigate Google Maps and fetch real-time busyness data for each attraction. The data is returned in a tabular format for easy interpretation.\n",
    "\n",
    "From a product perspective, this is one of the most compelling features: potential users can check in real time which places to visit and which to avoid based on current crowding levels, helping them find the best spot at any given moment. However, this functionality depends entirely on the previously created SQL database containing all attraction information for a given city. There is a symbiotic relationship here, where long-term data storage enables live data retrieval and short-term usage of the product.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7aa713",
   "metadata": {},
   "source": [
    "## Selenium implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae8ed7a",
   "metadata": {},
   "source": [
    "The Selenium implementation builds on the pre-filled SQL database. It centers around the parent scrape_peak_hours() function, which orchestrates the following:\n",
    "\n",
    "- *get_attraction_names(city, conn)*: Queries the connected database for all attractions matching the input city.\n",
    "- *get_current_busyness(driver, name, city)*: The core Selenium WebDriver script that identifies and extracts the current busyness bar for an attraction. Contains helpers:\n",
    "  - *_parse_busy_bar(aria)*: Parses busyness percentage from the bar's aria-label.\n",
    "  - *dismiss_google_consent(driver)*: Dismisses the Google Maps cookie consent popup.\n",
    "\n",
    "This modular design lets you launch a complete scraping pipeline with one line (scrape_peak_hours(\"Paris\", conn)) which processes all matching attractions after initializing the Selenium WebDriver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec1524a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.selenium_driver import make_driver\n",
    "from src.selenium_peak_hours import scrape_peak_hours\n",
    "\n",
    "busyness_data = {}  # global storage for results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c29efb6",
   "metadata": {},
   "source": [
    "The Selenium implementation builds on the pre-filled SQL database. It centers around the parent `scrape_peak_hours()` function, which orchestrates the following:\n",
    "\n",
    "- *get_attraction_names(city, conn)*: Queries the connected database for all attractions matching the input city.\n",
    "- *get_current_busyness(driver, name)*: The core Selenium WebDriver script that identifies and extracts the full-day busyness profile for each attraction. It contains helper functions:\n",
    "  - *_parse_busy_bar(aria)*: Parses busyness percentage values from the bar’s aria-label.\n",
    "  - *dismiss_google_consent(driver)*: Dismisses the Google Maps cookie consent popup.\n",
    "\n",
    "After initializing the Selenium WebDriver, a complete scraping pipeline can be launched with a single function call. The script sequentially processes all stored attractions for the selected city and stores the results in structured format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e499d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver started.\n",
      "Looking up 'Paris' in  DB...\n",
      "  Found 10 attractions: Eiffel Tower, Louvre Museum, Arc de Triomphe...\n",
      "\n",
      "  Searching: Eiffel Tower\n",
      "  Google consent dismissed.\n",
      "    Landed directly on place page.\n",
      "    Found 126 hourly bars.\n",
      "    ✓ Live now: 23%\n",
      "    ✓ Stored 18/24 hours.\n",
      "\n",
      "  Searching: Louvre Museum\n",
      "  No consent popup found.\n",
      "    Landed directly on place page.\n",
      "    Found 108 hourly bars.\n",
      "    ✓ Stored 18/24 hours.\n",
      "\n",
      "  Searching: Arc de Triomphe\n",
      "  No consent popup found.\n",
      "    Landed directly on place page.\n",
      "    Found 126 hourly bars.\n",
      "    ✓ Stored 18/24 hours.\n",
      "\n",
      "  Searching: Champ de Mars\n",
      "  No consent popup found.\n",
      "    Clicked top result from list.\n",
      "    Found 168 hourly bars.\n",
      "    ✓ Live now: 26%\n",
      "    ✓ Stored 24/24 hours.\n",
      "\n",
      "  Searching: Jardin du Luxembourg\n",
      "  No consent popup found.\n",
      "    Landed directly on place page.\n",
      "    No peak hours data available.\n",
      "\n",
      "  Searching: Tuileries Garden\n",
      "  No consent popup found.\n",
      "    Landed directly on place page.\n",
      "    Found 126 hourly bars.\n",
      "    ✓ Stored 18/24 hours.\n",
      "\n",
      "  Searching: Notre-Dame Cathedral of Paris\n",
      "  No consent popup found.\n",
      "    Landed directly on place page.\n",
      "    Found 126 hourly bars.\n",
      "    ✓ Stored 18/24 hours.\n",
      "\n",
      "  Searching: Palais Garnier\n",
      "  No consent popup found.\n",
      "    Landed directly on place page.\n",
      "    No peak hours data available.\n",
      "\n",
      "  Searching: Sainte-Chapelle\n",
      "  No consent popup found.\n",
      "    Landed directly on place page.\n",
      "    Found 126 hourly bars.\n",
      "    ✓ Stored 18/24 hours.\n",
      "\n",
      "  Searching: Place des Vosges\n",
      "  No consent popup found.\n",
      "    Landed directly on place page.\n",
      "    Found 168 hourly bars.\n",
      "    ✓ Live now: 16%\n",
      "    ✓ Stored 24/24 hours.\n",
      "\n",
      "======================================================\n",
      "  CURRENT BUSYNESS — Paris  (scraped at 22:49)\n",
      "======================================================\n",
      "  Eiffel Tower                                 63%\n",
      "  Louvre Museum                                0%\n",
      "  Arc de Triomphe                              44%\n",
      "  Champ de Mars                                66%\n",
      "  Jardin du Luxembourg                         N/A (no GM data)\n",
      "  Tuileries Garden                             0%\n",
      "  Notre-Dame Cathedral of Paris                0%\n",
      "  Palais Garnier                               N/A (no GM data)\n",
      "  Sainte-Chapelle                              0%\n",
      "  Place des Vosges                             21%\n",
      "Finished scraping.\n"
     ]
    }
   ],
   "source": [
    "import src.ui as ui\n",
    "\n",
    "driver = make_driver(headless=True)\n",
    "print(\"Driver started.\")\n",
    "\n",
    "scrape_peak_hours(ui.LAST_SEARCHED_CITY, conn, driver, busyness_data)\n",
    "\n",
    "driver.close()\n",
    "print(\"Finished scraping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b6d610",
   "metadata": {},
   "source": [
    "This section converts the scraped peak-hours data into a structured DataFrame for presentation. The `print_busyness_summary()` function extracts the current-hour crowdedness values from the stored results and formats them into a tabular overview.\n",
    "\n",
    "This step bridges live Selenium retrieval with interpretable output, enabling users to immediately assess which attractions are currently more or less crowded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6efdbf29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Paris  (scraped at 22:49) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attraction</th>\n",
       "      <th>Busy at 23:00</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eiffel Tower</td>\n",
       "      <td>0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Louvre Museum</td>\n",
       "      <td>0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arc de Triomphe</td>\n",
       "      <td>0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Champ de Mars</td>\n",
       "      <td>45%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jardin du Luxembourg</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tuileries Garden</td>\n",
       "      <td>0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Notre-Dame Cathedral of Paris</td>\n",
       "      <td>0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Palais Garnier</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sainte-Chapelle</td>\n",
       "      <td>0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Place des Vosges</td>\n",
       "      <td>17%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Attraction Busy at 23:00\n",
       "0                   Eiffel Tower            0%\n",
       "1                  Louvre Museum            0%\n",
       "2                Arc de Triomphe            0%\n",
       "3                  Champ de Mars           45%\n",
       "4           Jardin du Luxembourg           N/A\n",
       "5               Tuileries Garden            0%\n",
       "6  Notre-Dame Cathedral of Paris            0%\n",
       "7                 Palais Garnier           N/A\n",
       "8                Sainte-Chapelle            0%\n",
       "9               Place des Vosges           17%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "def print_busyness_summary(city: str = None):\n",
    "    \"\"\"Print a DataFrame of current-hour busyness. Omit city= for all cities.\"\"\"\n",
    "    cities = [city] if city else list(busyness_data.keys())\n",
    "    now_hour = datetime.datetime.now().hour\n",
    "\n",
    "    if not cities:\n",
    "        print(\"busyness_data is empty — run scrape_peak_hours() first.\")\n",
    "        return\n",
    "\n",
    "    for c in cities:\n",
    "        if c not in busyness_data:\n",
    "            print(f\"No data for '{c}'.\")\n",
    "            continue\n",
    "        entry = busyness_data[c]\n",
    "        rows = []\n",
    "        for name, hourly in entry[\"attractions\"].items():\n",
    "            pct = None if (hourly is None) else hourly[now_hour]\n",
    "            rows.append({\n",
    "                \"Attraction\": name,\n",
    "                f\"Busy at {now_hour:02d}:00\": f\"{pct}%\" if pct is not None else \"N/A\"\n",
    "            })\n",
    "        print(f\"\\n=== {c}  (scraped at {entry['scraped_at']}) ===\")\n",
    "        display(pd.DataFrame(rows))\n",
    "\n",
    "print_busyness_summary(LAST_SEARCHED_CITY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5302b3fc",
   "metadata": {},
   "source": [
    "## Results and Limitations\n",
    "From an overall project results perspective, we are very satisfied with the final outcomes.\n",
    "Following the implementation of the Google and TripAdvisor APIs, we successfully populated a solid SQL database. We felt that inserting a limited number of cities was sufficient for demonstration purposes, but the code is written in an interactive way such that, on demand, we can integrate our database with additional information for any desired city.\n",
    "We are also pleased with our implementation of Selenium to retrieve crowdedness information. Fetching data directly from the SQL database makes the process leaner and allows users to access this information at a later time, which is key to the system's functionality. Users can first explore attractions and activities, then decide later whether to run the Selenium script for real-time data. Fetching this data can be time-intensive, since we deliberately slowed the process to avoid potential scraping blocks, taking a conservative approach.\n",
    "\n",
    "This leads us to the project's current limitations.\n",
    "- First, rapid fetching of attraction crowdedness would be essential in a working product. Given the pedagogical nature of this project, we did not feel it was necessary to optimize speed or implement techniques to parallelize the process. However, we recognize this flaw, which would be critical to address for a deliverable product.\n",
    "- Second, crowdedness can only be checked for attractions in the current implementation. Since we use Google Maps to retrieve this information, we are limited to Google Maps attractions. Unfortunately, we cannot check activity crowdedness for TripAdvisor-sourced activities, as TripAdvisor does not supply users with such data.\n",
    "- Additionally, the current implementation requires using a VPN, connected to a country whose official language is english, to run the scraping script reliably. Google Maps interface language still changes based on geolocation, and simply adjusting the WebDriver headers has not been enough to override this behavior. In future iterations, more robust workarounds could be introduced, but given the course context and prototype nature of the project, we consider the present solution acceptable.\n",
    "- Finally, we must acknowledge the static nature of data inserted into the SQL database. For a more robust product, implementing a recurring update function would be fundamental. With the present structure, manually erasing all stored data and using the interactive UI to fetch it again would be necessary to 'update' the available information. This is costly, both computationally and time-wise. A better approach would be to implement regular checks to determine whether data has changed enough to warrant the expense of updating.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cded64",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "As a proof of concept for a product that could facilitate travel planning for tourists worldwide, we consider ourselves satisfied. The combined use of long-term data storage with short-term data retrieval through APIs and the Selenium package proved to be a powerful synergy, allowing flexibility, ease of access, and surprisingly decent timeliness. \n",
    "\n",
    "Of course, we acknowledge the aforementioned limitations. However, scaling this project with more computing power, parallelization of processes across multiple (virtual) machines, and strategic implementation of powerful libraries such as the Requests library could improve timeliness and make such a product a deliverable reality.\n",
    "\n",
    "Furthermore, we cannot ignore the pedagogical benefits of this project. Working with APIs and a robust package like Selenium on mainstream platforms such as Google Maps and TripAdvisor gave us valuable insights into data retrieval, web scraping, and the ideation of a working product. These are skills that are difficult to develop in an educational setting. We believe this experience will be highly beneficial for our professional growth.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22a0c76",
   "metadata": {},
   "source": [
    "### AI Usage Statement\n",
    "\n",
    "The conceptual design, system architecture, and implementation of this project were developed independently by the project team. All analytical content and report texts were written by the authors.\n",
    "\n",
    "Artificial Intelligence tools (ChatGPT, Gemini, Perplexity) were used solely to polish written text and to assist with coding support during implementation. The project README file constitutes the only component fully generated by AI for documentation purposes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
